{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "923261ab",
   "metadata": {},
   "source": [
    "# `.ipynb` to `.md` Converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a1d5b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook 1-Converter.ipynb to markdown\n",
      "[NbConvertApp] Writing 3173 bytes to 1-Converter.md\n",
      "[NbConvertApp] Converting notebook 1-Index.ipynb to markdown\n",
      "/opt/anaconda3/share/jupyter/nbconvert/templates/base/display_priority.j2:32: UserWarning: Your element with mimetype(s) dict_keys(['application/vnd.plotly.v1+json']) is not able to be represented.\n",
      "  {%- elif type == 'text/vnd.mermaid' -%}\n",
      "[NbConvertApp] Writing 50061 bytes to 1-Index.md\n",
      "[NbConvertApp] Converting notebook 2-Algebraic-and-Number-Theoritic.ipynb to markdown\n",
      "[NbConvertApp] Writing 5343 bytes to 2-Algebraic-and-Number-Theoritic.md\n",
      "[NbConvertApp] Converting notebook 2.1-Factoring.ipynb to markdown\n",
      "[NbConvertApp] Writing 8367 bytes to 2.1-Factoring.md\n",
      "[NbConvertApp] Converting notebook 2.10-Matrix Elements and Multiplicity Coefficients of Group Representations.ipynb to markdown\n",
      "[NbConvertApp] Writing 7397 bytes to 2.10-Matrix Elements and Multiplicity Coefficients of Group Representations.md\n",
      "[NbConvertApp] Converting notebook 2.11-Verifying Matrix Products.ipynb to markdown\n",
      "[NbConvertApp] Writing 6240 bytes to 2.11-Verifying Matrix Products.md\n",
      "[NbConvertApp] Converting notebook 2.12-Subset-sum.ipynb to markdown\n",
      "[NbConvertApp] Writing 6380 bytes to 2.12-Subset-sum.md\n",
      "[NbConvertApp] Converting notebook 2.13-Decoding.ipynb to markdown\n",
      "[NbConvertApp] Writing 6648 bytes to 2.13-Decoding.md\n",
      "[NbConvertApp] Converting notebook 2.14-Quantum Cryptanalysis.ipynb to markdown\n",
      "[NbConvertApp] Writing 11 bytes to 2.14-Quantum Cryptanalysis.md\n",
      "[NbConvertApp] Converting notebook 2.2-Discrete-log.ipynb to markdown\n",
      "[NbConvertApp] Writing 7143 bytes to 2.2-Discrete-log.md\n",
      "[NbConvertApp] Converting notebook 2.3-Pell's-Equation.ipynb to markdown\n",
      "[NbConvertApp] Writing 6517 bytes to 2.3-Pell's-Equation.md\n",
      "[NbConvertApp] Converting notebook 2.4-Principal-Ideal.ipynb to markdown\n",
      "[NbConvertApp] Writing 5231 bytes to 2.4-Principal-Ideal.md\n",
      "[NbConvertApp] Converting notebook 2.5-Unit-Group.ipynb to markdown\n",
      "[NbConvertApp] Writing 6196 bytes to 2.5-Unit-Group.md\n",
      "[NbConvertApp] Converting notebook 2.6-Class-Group.ipynb to markdown\n",
      "[NbConvertApp] Writing 6854 bytes to 2.6-Class-Group.md\n",
      "[NbConvertApp] Converting notebook 2.7-Gauss-Sums.ipynb to markdown\n",
      "[NbConvertApp] Writing 7325 bytes to 2.7-Gauss-Sums.md\n",
      "[NbConvertApp] Converting notebook 2.8-Primality-Proving.ipynb to markdown\n",
      "[NbConvertApp] Writing 6637 bytes to 2.8-Primality-Proving.md\n",
      "[NbConvertApp] Converting notebook 2.9-Solving-Exponential-Congruences.ipynb to markdown\n",
      "[NbConvertApp] Writing 6652 bytes to 2.9-Solving-Exponential-Congruences.md\n",
      "[NbConvertApp] Converting notebook 3-OracularAlgprithms.ipynb to markdown\n",
      "[NbConvertApp] Writing 8921 bytes to 3-OracularAlgprithms.md\n",
      "[NbConvertApp] Converting notebook 3.1-Searching.ipynb to markdown\n",
      "[NbConvertApp] Writing 6542 bytes to 3.1-Searching.md\n",
      "[NbConvertApp] Converting notebook 3.10-Ordered Search.ipynb to markdown\n",
      "[NbConvertApp] Writing 6395 bytes to 3.10-Ordered Search.md\n",
      "[NbConvertApp] Converting notebook 3.11-Graph Properties in the Adjacency Matrix Model.ipynb to markdown\n",
      "[NbConvertApp] Writing 6332 bytes to 3.11-Graph Properties in the Adjacency Matrix Model.md\n",
      "[NbConvertApp] Converting notebook 3.12-Graph Properties in the Adjacency List Model.ipynb to markdown\n",
      "[NbConvertApp] Writing 6131 bytes to 3.12-Graph Properties in the Adjacency List Model.md\n",
      "[NbConvertApp] Converting notebook 3.13-Welded Tree.ipynb to markdown\n",
      "[NbConvertApp] Writing 5911 bytes to 3.13-Welded Tree.md\n",
      "[NbConvertApp] Converting notebook 3.14-Collision Finding and Element Distinctness.ipynb to markdown\n",
      "[NbConvertApp] Writing 6205 bytes to 3.14-Collision Finding and Element Distinctness.md\n",
      "[NbConvertApp] Converting notebook 3.15-Graph Collision.ipynb to markdown\n",
      "[NbConvertApp] Writing 6263 bytes to 3.15-Graph Collision.md\n",
      "[NbConvertApp] Converting notebook 3.16-Matrix Commutativity.ipynb to markdown\n",
      "[NbConvertApp] Writing 6103 bytes to 3.16-Matrix Commutativity.md\n",
      "[NbConvertApp] Converting notebook 3.17-Group Commutativity.ipynb to markdown\n",
      "[NbConvertApp] Writing 5707 bytes to 3.17-Group Commutativity.md\n",
      "[NbConvertApp] Converting notebook 3.18-Hidden Nonlinear Structures.ipynb to markdown\n",
      "[NbConvertApp] Writing 6186 bytes to 3.18-Hidden Nonlinear Structures.md\n",
      "[NbConvertApp] Converting notebook 3.19-Center of Radial Function.ipynb to markdown\n",
      "[NbConvertApp] Writing 6659 bytes to 3.19-Center of Radial Function.md\n",
      "[NbConvertApp] Converting notebook 3.2-Abelian Hidden Subgroup.ipynb to markdown\n",
      "[NbConvertApp] Writing 6035 bytes to 3.2-Abelian Hidden Subgroup.md\n",
      "[NbConvertApp] Converting notebook 3.20-Group Order and Membership.ipynb to markdown\n",
      "[NbConvertApp] Writing 5595 bytes to 3.20-Group Order and Membership.md\n",
      "[NbConvertApp] Converting notebook 3.21-Group Isomorphism.ipynb to markdown\n",
      "[NbConvertApp] Writing 6202 bytes to 3.21-Group Isomorphism.md\n",
      "[NbConvertApp] Converting notebook 3.22-Statistical Difference.ipynb to markdown\n",
      "[NbConvertApp] Writing 6234 bytes to 3.22-Statistical Difference.md\n",
      "[NbConvertApp] Converting notebook 3.23-Finite Rings and Ideals.ipynb to markdown\n",
      "[NbConvertApp] Writing 6238 bytes to 3.23-Finite Rings and Ideals.md\n",
      "[NbConvertApp] Converting notebook 3.24-Counterfeit Coins.ipynb to markdown\n",
      "[NbConvertApp] Writing 6332 bytes to 3.24-Counterfeit Coins.md\n",
      "[NbConvertApp] Converting notebook 3.25-Matrix Rank.ipynb to markdown\n",
      "[NbConvertApp] Writing 6157 bytes to 3.25-Matrix Rank.md\n",
      "[NbConvertApp] Converting notebook 3.26-Matrix Multiplication over Semirings.ipynb to markdown\n",
      "[NbConvertApp] Writing 6650 bytes to 3.26-Matrix Multiplication over Semirings.md\n",
      "[NbConvertApp] Converting notebook 3.27-Subset finding.ipynb to markdown\n",
      "[NbConvertApp] Writing 6049 bytes to 3.27-Subset finding.md\n",
      "[NbConvertApp] Converting notebook 3.28-Search with Wildcards.ipynb to markdown\n",
      "[NbConvertApp] Writing 6033 bytes to 3.28-Search with Wildcards.md\n",
      "[NbConvertApp] Converting notebook 3.29-Network flows.ipynb to markdown\n",
      "[NbConvertApp] Writing 6390 bytes to 3.29-Network flows.md\n",
      "[NbConvertApp] Converting notebook 3.3-Non-Abelian Hidden Subgroup.ipynb to markdown\n",
      "[NbConvertApp] Writing 6594 bytes to 3.3-Non-Abelian Hidden Subgroup.md\n",
      "[NbConvertApp] Converting notebook 3.30-Electrical Resistance.ipynb to markdown\n",
      "[NbConvertApp] Writing 6219 bytes to 3.30-Electrical Resistance.md\n",
      "[NbConvertApp] Converting notebook 3.31-Junta Testing and Group Testing.ipynb to markdown\n",
      "[NbConvertApp] Writing 6183 bytes to 3.31-Junta Testing and Group Testing.md\n",
      "[NbConvertApp] Converting notebook 3.4-Bernstein-Vazirani.ipynb to markdown\n",
      "[NbConvertApp] Writing 6189 bytes to 3.4-Bernstein-Vazirani.md\n",
      "[NbConvertApp] Converting notebook 3.5-Deutsch-Jozsa.ipynb to markdown\n",
      "[NbConvertApp] Writing 6648 bytes to 3.5-Deutsch-Jozsa.md\n",
      "[NbConvertApp] Converting notebook 3.6-Formula Evaluation.ipynb to markdown\n",
      "[NbConvertApp] Writing 6123 bytes to 3.6-Formula Evaluation.md\n",
      "[NbConvertApp] Converting notebook 3.7-Hidden Shift.ipynb to markdown\n",
      "[NbConvertApp] Writing 6442 bytes to 3.7-Hidden Shift.md\n",
      "[NbConvertApp] Converting notebook 3.8-Polynomial interpolation.ipynb to markdown\n",
      "[NbConvertApp] Writing 6621 bytes to 3.8-Polynomial interpolation.md\n",
      "[NbConvertApp] Converting notebook 3.9-Pattern matching.ipynb to markdown\n",
      "[NbConvertApp] Writing 6446 bytes to 3.9-Pattern matching.md\n",
      "[NbConvertApp] Converting notebook 4-Approximation-Simulation.ipynb to markdown\n",
      "[NbConvertApp] Writing 15336 bytes to 4-Approximation-Simulation.md\n",
      "[NbConvertApp] Converting notebook 4.1-Simulating Quantum Hamiltonian Dynamics.ipynb to markdown\n",
      "[NbConvertApp] Writing 6519 bytes to 4.1-Simulating Quantum Hamiltonian Dynamics.md\n",
      "[NbConvertApp] Converting notebook 4.10-Matrix Powers.ipynb to markdown\n",
      "[NbConvertApp] Writing 6368 bytes to 4.10-Matrix Powers.md\n",
      "[NbConvertApp] Converting notebook 4.11-Probabilistic Sampling.ipynb to markdown\n",
      "[NbConvertApp] Writing 7084 bytes to 4.11-Probabilistic Sampling.md\n",
      "[NbConvertApp] Converting notebook 4.2-Preparing Eigenstates and Thermal States.ipynb to markdown\n",
      "[NbConvertApp] Writing 6730 bytes to 4.2-Preparing Eigenstates and Thermal States.md\n",
      "[NbConvertApp] Converting notebook 4.3-Knot Invariants.ipynb to markdown\n",
      "[NbConvertApp] Writing 6475 bytes to 4.3-Knot Invariants.md\n",
      "[NbConvertApp] Converting notebook 4.4-Three-manifold Invariants.ipynb to markdown\n",
      "[NbConvertApp] Writing 6179 bytes to 4.4-Three-manifold Invariants.md\n",
      "[NbConvertApp] Converting notebook 4.5-Partition Functions.ipynb to markdown\n",
      "[NbConvertApp] Writing 6826 bytes to 4.5-Partition Functions.md\n",
      "[NbConvertApp] Converting notebook 4.6-Zeta Functions.ipynb to markdown\n",
      "[NbConvertApp] Writing 6278 bytes to 4.6-Zeta Functions.md\n",
      "[NbConvertApp] Converting notebook 4.7-Weight Enumerators.ipynb to markdown\n",
      "[NbConvertApp] Writing 6176 bytes to 4.7-Weight Enumerators.md\n",
      "[NbConvertApp] Converting notebook 4.8-Simulated Annealing.ipynb to markdown\n",
      "[NbConvertApp] Writing 6033 bytes to 4.8-Simulated Annealing.md\n",
      "[NbConvertApp] Converting notebook 4.9-String Rewriting.ipynb to markdown\n",
      "[NbConvertApp] Writing 6377 bytes to 4.9-String Rewriting.md\n",
      "[NbConvertApp] Converting notebook 5-Optimization-and-ML.ipynb to markdown\n",
      "[NbConvertApp] Writing 15776 bytes to 5-Optimization-and-ML.md\n",
      "[NbConvertApp] Converting notebook 5.1-Polynomial Quantum Speedups for Constraint Satisfaction Problems.ipynb to markdown\n",
      "[NbConvertApp] Writing 6299 bytes to 5.1-Polynomial Quantum Speedups for Constraint Satisfaction Problems.md\n",
      "[NbConvertApp] Converting notebook 5.10-Machine Learning.ipynb to markdown\n",
      "[NbConvertApp] Writing 7509 bytes to 5.10-Machine Learning.md\n",
      "[NbConvertApp] Converting notebook 5.11-Tensor Principal Component Analysis.ipynb to markdown\n",
      "[NbConvertApp] Writing 6916 bytes to 5.11-Tensor Principal Component Analysis.md\n",
      "[NbConvertApp] Converting notebook 5.12-Solving Linear Differential Equations.ipynb to markdown\n",
      "[NbConvertApp] Writing 6476 bytes to 5.12-Solving Linear Differential Equations.md\n",
      "[NbConvertApp] Converting notebook 5.13-Solving Nonlinear Differential Equations.ipynb to markdown\n",
      "[NbConvertApp] Writing 6652 bytes to 5.13-Solving Nonlinear Differential Equations.md\n",
      "[NbConvertApp] Converting notebook 5.14-Quantum Dynamic Programming for path-in-the-hypercube.ipynb to markdown\n",
      "[NbConvertApp] Writing 6708 bytes to 5.14-Quantum Dynamic Programming for path-in-the-hypercube.md\n",
      "[NbConvertApp] Converting notebook 5.15-Computing the Principal Eigenvector.ipynb to markdown\n",
      "[NbConvertApp] Writing 6645 bytes to 5.15-Computing the Principal Eigenvector.md\n",
      "[NbConvertApp] Converting notebook 5.16-Approximating Nash Equilibria.ipynb to markdown\n",
      "[NbConvertApp] Writing 6315 bytes to 5.16-Approximating Nash Equilibria.md\n",
      "[NbConvertApp] Converting notebook 5.17-Lattice Problems by Filtering.ipynb to markdown\n",
      "[NbConvertApp] Writing 6456 bytes to 5.17-Lattice Problems by Filtering.md\n",
      "[NbConvertApp] Converting notebook 5.18-Double-bracket quantum algorithms.ipynb to markdown\n",
      "[NbConvertApp] Writing 6164 bytes to 5.18-Double-bracket quantum algorithms.md\n",
      "[NbConvertApp] Converting notebook 5.2-Adiabatic Algorithms.ipynb to markdown\n",
      "[NbConvertApp] Writing 6445 bytes to 5.2-Adiabatic Algorithms.md\n",
      "[NbConvertApp] Converting notebook 5.3-Quantum Approximate Optimization.ipynb to markdown\n",
      "[NbConvertApp] Writing 6746 bytes to 5.3-Quantum Approximate Optimization.md\n",
      "[NbConvertApp] Converting notebook 5.4-Gradient Estimation and Learning Polynomials.ipynb to markdown\n",
      "[NbConvertApp] Writing 6428 bytes to 5.4-Gradient Estimation and Learning Polynomials.md\n",
      "[NbConvertApp] Converting notebook 5.5-Semidefinite Programming.ipynb to markdown\n",
      "[NbConvertApp] Writing 6672 bytes to 5.5-Semidefinite Programming.md\n",
      "[NbConvertApp] Converting notebook 5.6-Convex Optimization.ipynb to markdown\n",
      "[NbConvertApp] Writing 6146 bytes to 5.6-Convex Optimization.md\n",
      "[NbConvertApp] Converting notebook 5.7-Optimization by Decoded Quantum Interferometry.ipynb to markdown\n",
      "[NbConvertApp] Writing 6174 bytes to 5.7-Optimization by Decoded Quantum Interferometry.md\n",
      "[NbConvertApp] Converting notebook 5.8-Linear Systems.ipynb to markdown\n",
      "[NbConvertApp] Writing 6864 bytes to 5.8-Linear Systems.md\n",
      "[NbConvertApp] Converting notebook 5.9-Estimating Determinants and Other Spectral Sums.ipynb to markdown\n",
      "[NbConvertApp] Writing 6210 bytes to 5.9-Estimating Determinants and Other Spectral Sums.md\n",
      "[NbConvertApp] Converting notebook X-FutureAlgorithms.ipynb to markdown\n",
      "[NbConvertApp] Writing 49118 bytes to X-FutureAlgorithms.md\n"
     ]
    }
   ],
   "source": [
    "! jupyter nbconvert --to markdown *.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0164ee5d",
   "metadata": {},
   "source": [
    "## Combine all `.md` file to create `book.md`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c8b0f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat *.md > book.md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2afe66",
   "metadata": {},
   "source": [
    "## Remove long url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bdd13d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete!\n",
      "Found and converted 127 long URLs.\n",
      "Cleaned file saved as 'book_cleaned.md'.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "def shorten_long_urls_in_markdown(input_file, output_file, url_length_threshold=80):\n",
    "    \"\"\"\n",
    "    Converts long inline Markdown links to reference-style links.\n",
    "\n",
    "    Args:\n",
    "        input_file (str): Path to the source Markdown file.\n",
    "        output_file (str): Path to save the cleaned Markdown file.\n",
    "        url_length_threshold (int): The minimum length for a URL to be considered \"long\".\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(input_file, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{input_file}' was not found.\")\n",
    "        return\n",
    "\n",
    "    # Regex to find all inline markdown links: [text](url)\n",
    "    # It correctly handles nested brackets in the link text.\n",
    "    inline_link_regex = re.compile(r'\\[((?:[^\\[\\]]|\\[[^\\]]*\\])*)\\]\\(([^)]+)\\)')\n",
    "\n",
    "    references = {}\n",
    "    ref_counter = 1\n",
    "    \n",
    "    # This function will be called for each match found by re.sub\n",
    "    def replace_link(match):\n",
    "        nonlocal ref_counter\n",
    "        link_text = match.group(1)\n",
    "        url = match.group(2)\n",
    "\n",
    "        # Check if the URL is long and not already a reference placeholder\n",
    "        if len(url) > url_length_threshold and not url.startswith('#'):\n",
    "            # Check if we have already created a reference for this URL\n",
    "            if url in references:\n",
    "                ref_id = references[url]\n",
    "            else:\n",
    "                ref_id = f\"ref{ref_counter}\"\n",
    "                references[url] = ref_id\n",
    "                ref_counter += 1\n",
    "            \n",
    "            # Return the new reference-style link\n",
    "            return f\"[{link_text}][{ref_id}]\"\n",
    "        else:\n",
    "            # If the URL is not long, leave the link as is\n",
    "            return match.group(0)\n",
    "\n",
    "    # Perform the replacement across the entire file content\n",
    "    new_content = inline_link_regex.sub(replace_link, content)\n",
    "\n",
    "    # Build the reference list to append at the end\n",
    "    if references:\n",
    "        # Invert the dictionary to sort by ref_id number\n",
    "        sorted_refs = sorted(references.items(), key=lambda item: int(item[1].replace('ref', '')))\n",
    "        \n",
    "        ref_list_str = \"\\n\\n\" + \"=\"*80 + \"\\n\"\n",
    "        ref_list_str += \"<!-- Link References -->\\n\"\n",
    "        ref_list_str += \"=\"*80 + \"\\n\"\n",
    "        \n",
    "        for url, ref_id in sorted_refs:\n",
    "            ref_list_str += f\"[{ref_id}]: {url}\\n\"\n",
    "            \n",
    "        new_content += ref_list_str\n",
    "\n",
    "    # Write the cleaned content to the output file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(new_content)\n",
    "\n",
    "    print(f\"Processing complete!\")\n",
    "    print(f\"Found and converted {len(references)} long URLs.\")\n",
    "    print(f\"Cleaned file saved as '{output_file}'.\")\n",
    "\n",
    "\n",
    "# --- Main execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    INPUT_FILENAME = \"book.md\"\n",
    "    OUTPUT_FILENAME = \"book_cleaned.md\"\n",
    "    \n",
    "    # You can change the threshold for what is considered a \"long\" URL\n",
    "    URL_THRESHOLD = 80 \n",
    "\n",
    "    shorten_long_urls_in_markdown(INPUT_FILENAME, OUTPUT_FILENAME, URL_THRESHOLD)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8605b575",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42dc2b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: Converted 550 references to references_extracted.csv\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import csv\n",
    "\n",
    "def parse_references_with_date(raw_text, output_filename=\"references_extracted.csv\"):\n",
    "    # Split text by lines and remove empty ones\n",
    "    lines = [line.strip() for line in raw_text.split('\\n') if line.strip()]\n",
    "    \n",
    "    data = []\n",
    "    current_entry = {}\n",
    "    \n",
    "    # Pattern to find the Reference ID (e.g., \"1\", \"105\")\n",
    "    id_pattern = re.compile(r'^\\d+$')\n",
    "    \n",
    "    # Pattern to find a Year (1900-2099)\n",
    "    date_pattern = re.compile(r'\\b(19\\d{2}|20\\d{2})\\b')\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(lines):\n",
    "        line = lines[i]\n",
    "        \n",
    "        if id_pattern.match(line):\n",
    "            # Save previous entry before starting a new one\n",
    "            if current_entry:\n",
    "                data.append(current_entry)\n",
    "            \n",
    "            current_entry = {\n",
    "                'ID': line,\n",
    "                'Date': '',        # New Column\n",
    "                'Authors': '',\n",
    "                'Title': '',\n",
    "                'Publication': '',\n",
    "                'arXiv': ''\n",
    "            }\n",
    "            \n",
    "            # Heuristic: Author is usually the line after ID\n",
    "            if i + 1 < len(lines):\n",
    "                current_entry['Authors'] = lines[i+1]\n",
    "            \n",
    "            # Heuristic: Title is usually the line after Authors\n",
    "            if i + 2 < len(lines):\n",
    "                current_entry['Title'] = lines[i+2]\n",
    "                \n",
    "            # Heuristic: Scan the remaining lines for Publication/arXiv/Date info\n",
    "            # until we hit the next ID number\n",
    "            pub_parts = []\n",
    "            arxiv_parts = []\n",
    "            \n",
    "            j = i + 3\n",
    "            while j < len(lines) and not id_pattern.match(lines[j]):\n",
    "                content = lines[j]\n",
    "                \n",
    "                # Check for arXiv identifiers\n",
    "                if \"arXiv\" in content or \"quant-ph\" in content or \"cond-mat\" in content:\n",
    "                    clean_arxiv = content.replace('[', '').replace(']', '')\n",
    "                    arxiv_parts.append(clean_arxiv)\n",
    "                else:\n",
    "                    pub_parts.append(content)\n",
    "                j += 1\n",
    "            \n",
    "            # Join the gathered lines\n",
    "            full_pub_string = \" \".join(pub_parts)\n",
    "            full_arxiv_string = \"; \".join(arxiv_parts)\n",
    "            \n",
    "            current_entry['Publication'] = full_pub_string\n",
    "            current_entry['arXiv'] = full_arxiv_string\n",
    "            \n",
    "            # --- EXTRACT DATE LOGIC ---\n",
    "            # 1. Look for year in Publication string first\n",
    "            years = date_pattern.findall(full_pub_string)\n",
    "            \n",
    "            # 2. If not found, look in arXiv string\n",
    "            if not years:\n",
    "                years = date_pattern.findall(full_arxiv_string)\n",
    "            \n",
    "            # 3. If not found, look in the Title (rare, but happens in proceedings)\n",
    "            if not years:\n",
    "                 years = date_pattern.findall(current_entry['Title'])\n",
    "\n",
    "            # If multiple years appear, the last one is usually the publication year\n",
    "            if years:\n",
    "                current_entry['Date'] = years[-1]\n",
    "            # --------------------------\n",
    "            \n",
    "            i = j\n",
    "        else:\n",
    "            i += 1\n",
    "\n",
    "    # Append the final entry\n",
    "    if current_entry:\n",
    "        data.append(current_entry)\n",
    "\n",
    "    # Define columns (Date is now the 2nd column)\n",
    "    keys = ['ID', 'Date', 'Authors', 'Title', 'Publication', 'arXiv']\n",
    "    \n",
    "    with open(output_filename, 'w', newline='', encoding='utf-8') as output_file:\n",
    "        dict_writer = csv.DictWriter(output_file, fieldnames=keys)\n",
    "        dict_writer.writeheader()\n",
    "        dict_writer.writerows(data)\n",
    "        \n",
    "    print(f\"Success: Converted {len(data)} references to {output_filename}\")\n",
    "\n",
    "# HOW TO RUN:\n",
    "# 1. Paste your text into a file named 'refs.txt'\n",
    "# 2. Run the following block:\n",
    "with open('refs.txt', 'r', encoding='utf-8') as f:\n",
    "     parse_references_with_date(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94f0591",
   "metadata": {},
   "source": [
    "# Download all Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "385ada4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting arxiv\n",
      "  Downloading arxiv-2.3.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.11/site-packages (2.3.2)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.11/site-packages (4.66.4)\n",
      "Requirement already satisfied: feedparser~=6.0.10 in /opt/anaconda3/lib/python3.11/site-packages (from arxiv) (6.0.11)\n",
      "Requirement already satisfied: requests~=2.32.0 in /opt/anaconda3/lib/python3.11/site-packages (from arxiv) (2.32.5)\n",
      "Requirement already satisfied: sgmllib3k in /opt/anaconda3/lib/python3.11/site-packages (from feedparser~=6.0.10->arxiv) (1.0.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests~=2.32.0->arxiv) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.11/site-packages (from requests~=2.32.0->arxiv) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests~=2.32.0->arxiv) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from requests~=2.32.0->arxiv) (2024.7.4)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /opt/anaconda3/lib/python3.11/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading arxiv-2.3.1-py3-none-any.whl (11 kB)\n",
      "Installing collected packages: arxiv\n",
      "Successfully installed arxiv-2.3.1\n"
     ]
    }
   ],
   "source": [
    "!pip install arxiv pandas tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9486fbbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 550 rows from references.csv\n",
      "Found 459 papers to download.\n",
      "Starting download...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 71/459 [04:34<35:44,  5.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error downloading quant-h/0211140: Page request resulted in HTTP 400 (https://export.arxiv.org/api/query?search_query=&id_list=quant-h%2F0211140&sortBy=relevance&sortOrder=descending&start=0&max_results=100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 459/459 [31:57<00:00,  4.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done! Check the 'papers' folder.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import arxiv\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configuration\n",
    "INPUT_CSV = 'references.csv'\n",
    "OUTPUT_FOLDER = 'papers'\n",
    "# arXiv requires a delay to prevent blocking (approx 3s is polite)\n",
    "DELAY_SECONDS = 3 \n",
    "\n",
    "def sanitize_filename(filename):\n",
    "    \"\"\"Removes illegal characters for file names.\"\"\"\n",
    "    return re.sub(r'[\\\\/*?:\"<>|]', \"\", filename)[:150] # Limit length\n",
    "\n",
    "def extract_arxiv_ids(text):\n",
    "    \"\"\"\n",
    "    Extracts arXiv IDs from a string. \n",
    "    Handles formats like:\n",
    "    - arXiv:quant-ph/9703054\n",
    "    - quant-ph/9703054\n",
    "    - arXiv:0705.2784\n",
    "    - 0705.2784\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    \n",
    "    # Regex for both old (category/number) and new (dot number) formats\n",
    "    # It looks for patterns that resemble arXiv IDs\n",
    "    patterns = [\n",
    "        r'([a-z\\-]+/\\d{7})',      # Old format (e.g., quant-ph/9703054)\n",
    "        r'(\\d{4}\\.\\d{4,5})'       # New format (e.g., 0705.2784)\n",
    "    ]\n",
    "    \n",
    "    found_ids = []\n",
    "    for p in patterns:\n",
    "        matches = re.findall(p, text)\n",
    "        found_ids.extend(matches)\n",
    "        \n",
    "    return list(set(found_ids)) # Remove duplicates\n",
    "\n",
    "def main():\n",
    "    # 1. Create output directory\n",
    "    if not os.path.exists(OUTPUT_FOLDER):\n",
    "        os.makedirs(OUTPUT_FOLDER)\n",
    "        print(f\"Created folder: {OUTPUT_FOLDER}\")\n",
    "\n",
    "    # 2. Read CSV\n",
    "    try:\n",
    "        df = pd.read_csv(INPUT_CSV)\n",
    "        print(f\"Loaded {len(df)} rows from {INPUT_CSV}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Could not find {INPUT_CSV}. Make sure it's in the same folder.\")\n",
    "        return\n",
    "\n",
    "    # 3. Collect all IDs\n",
    "    tasks = []\n",
    "    for index, row in df.iterrows():\n",
    "        raw_text = row.get('arXiv', '')\n",
    "        ids = extract_arxiv_ids(raw_text)\n",
    "        \n",
    "        if ids:\n",
    "            # Just take the first valid ID found in the cell\n",
    "            tasks.append(ids[0])\n",
    "        else:\n",
    "            # Optional: Log missing IDs\n",
    "            # print(f\"Skipping Row {index+1}: No valid arXiv ID found.\")\n",
    "            pass\n",
    "\n",
    "    print(f\"Found {len(tasks)} papers to download.\")\n",
    "    \n",
    "    # 4. Initialize arXiv Client\n",
    "    client = arxiv.Client(\n",
    "        page_size=100,\n",
    "        delay_seconds=3,\n",
    "        num_retries=3\n",
    "    )\n",
    "\n",
    "    # 5. Process downloads\n",
    "    # We process in chunks or one-by-one. \n",
    "    # Querying the API with the ID gets us the correct PDF URL.\n",
    "    \n",
    "    print(\"Starting download...\")\n",
    "    \n",
    "    # We use tqdm for a progress bar\n",
    "    for paper_id in tqdm(tasks):\n",
    "        try:\n",
    "            # Fetch paper details from arXiv API\n",
    "            search = arxiv.Search(id_list=[paper_id])\n",
    "            paper = next(client.results(search))\n",
    "            \n",
    "            # Construct clean filename: ID_Title.pdf\n",
    "            safe_title = sanitize_filename(paper.title)\n",
    "            safe_id = sanitize_filename(paper_id)\n",
    "            filename = f\"{safe_id}_{safe_title}.pdf\"\n",
    "            filepath = os.path.join(OUTPUT_FOLDER, filename)\n",
    "            \n",
    "            # Check if file already exists to skip\n",
    "            if os.path.exists(filepath):\n",
    "                continue\n",
    "            \n",
    "            # Download\n",
    "            paper.download_pdf(dirpath=OUTPUT_FOLDER, filename=filename)\n",
    "            \n",
    "            # Polite delay\n",
    "            time.sleep(DELAY_SECONDS)\n",
    "            \n",
    "        except StopIteration:\n",
    "            print(f\"\\nWarning: ID {paper_id} not found on arXiv.\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError downloading {paper_id}: {e}\")\n",
    "\n",
    "    print(f\"\\nDone! Check the '{OUTPUT_FOLDER}' folder.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
